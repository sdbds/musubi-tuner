> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Kandinsky 5

## Overview / æ¦‚è¦

This is an unofficial training and inference script for [Kandinsky 5](https://github.com/ai-forever/Kandinsky-5). The features are as follows:

- fp8 support and memory reduction by block swap
- Inference without installing Flash attention (using PyTorch's scaled dot product attention)
- LoRA training for text-to-video (T2V) models

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>

[Kandinsky 5](https://github.com/ai-forever/Kandinsky-5) ã®éå…¬å¼ã®å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ï¼ˆT2Vï¼‰ãƒ¢ãƒ‡ãƒ«ã®LoRAå­¦ç¿’

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the model weights from the [Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50) on Hugging Face.

### DiT Model / DiTãƒ¢ãƒ‡ãƒ«

Download the DiT checkpoint from one of the following repositories:

**Lite models (2B parameters):**
- [Kandinsky-5.0-T2V-Lite-sft-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-sft-5s) - SFT model, highest quality
- [Kandinsky-5.0-T2V-Lite-sft-10s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-sft-10s) - 10 second videos
- [Kandinsky-5.0-T2V-Lite-pretrain-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-pretrain-5s) - Pretrain model for fine-tuning
- [Kandinsky-5.0-T2V-Lite-nocfg-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-nocfg-5s) - CFG-distilled, 2x faster
- [Kandinsky-5.0-T2V-Lite-distilled16steps-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-distilled16steps-5s) - Diffusion-distilled, 6x faster
- [Kandinsky-5.0-I2V-Lite-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-I2V-Lite-5s) - Image-to-Video

Download the `.safetensors` file (e.g., `kandinsky5lite_t2v_sft_5s.safetensors`).

### VAE

Kandinsky 5 uses the HunyuanVideo 3D VAE. Download `diffusion_pytorch_model.safetensors` (or `pytorch_model.pt`) from:
https://huggingface.co/tencent/HunyuanVideo/tree/main/hunyuan-video-t2v-720p/vae

### Text Encoders / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

Kandinsky 5 uses Qwen2.5-VL and CLIP for text encoding.

**Qwen2.5-VL**: Download from https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct (or use the path to your local Qwen2.5-VL model)

**CLIP**: Download `clip_l.safetensors` from https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files/text_encoders and place it in a directory (e.g., `text_encoder2/clip_l.safetensors`)

### Directory Structure / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

Place them in your chosen directory structure:

```
weights/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ kandinsky5lite_t2v_sft_5s.safetensors
â”œâ”€â”€ vae/
â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”œâ”€â”€ text_encoder/
â”‚   â””â”€â”€ (Qwen2.5-VL files)
â””â”€â”€ text_encoder2/
    â””â”€â”€ clip_l.safetensors
```

<details>
<summary>æ—¥æœ¬èª</summary>

Hugging Faceã®[Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50)ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**DiTãƒ¢ãƒ‡ãƒ«**: ä¸Šè¨˜ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰`.safetensors`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**VAE**: Kandinsky 5ã¯HunyuanVideo 3D VAEã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰`diffusion_pytorch_model.safetensors`ï¼ˆã¾ãŸã¯`pytorch_model.pt`ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**: Qwen2.5-VLã¨CLIPã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

</details>

## Available Tasks / åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯

The `--task` option specifies the model configuration. Available tasks:

**Lite models (2B parameters):**

| Task | Description | Resolution | Notes |
|------|-------------|------------|-------|
| `k5-lite-t2i-hd` | Lite T2I | 1024 | Image generation |
| `k5-lite-i2i-hd` | Lite I2I | 1024 | Image-to-image |
| `k5-lite-t2v-5s-sd` | Lite T2V 5s | 512 | SFT model |
| `k5-lite-t2v-10s-sd` | Lite T2V 10s | 512 | SFT model |
| `k5-lite-i2v-5s-sd` | Lite I2V 5s | 512 | Image-to-video |
| `k5-lite-t2v-5s-distil-sd` | Lite T2V 5s Distilled | 512 | 16 steps, faster |
| `k5-lite-t2v-10s-distil-sd` | Lite T2V 10s Distilled | 512 | 16 steps, faster |
| `k5-lite-t2v-5s-nocfg-sd` | Lite T2V 5s No-CFG | 512 | CFG-distilled |
| `k5-lite-t2v-10s-nocfg-sd` | Lite T2V 10s No-CFG | 512 | CFG-distilled |
| `k5-lite-t2v-5s-pretrain-sd` | Lite T2V 5s Pretrain | 512 | For fine-tuning |
| `k5-lite-t2v-10s-pretrain-sd` | Lite T2V 10s Pretrain | 512 | For fine-tuning |

**Pro models (19B parameters):**

| Task | Description | Resolution |
|------|-------------|------------|
| `k5-pro-t2v-5s-sd` | Pro T2V 5s SD | 512 |
| `k5-pro-t2v-5s-hd` | Pro T2V 5s HD | 1024 |
| `k5-pro-t2v-10s-sd` | Pro T2V 10s SD | 512 |
| `k5-pro-t2v-10s-hd` | Pro T2V 10s HD | 1024 |
| `k5-pro-i2v-5s-sd` | Pro I2V 5s SD | 512 |
| `k5-pro-i2v-5s-hd` | Pro I2V 5s HD | 1024 |

<details>
<summary>æ—¥æœ¬èª</summary>

`--task`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æŒ‡å®šã—ã¾ã™ã€‚åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã¯ä¸Šè¨˜ã®è¡¨ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

**Liteãƒ¢ãƒ‡ãƒ« (2Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)**: å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

**Proãƒ¢ãƒ‡ãƒ« (19Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)**: é«˜å“è³ªãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

</details>

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Pre-caching is required before training. This involves caching both latents and text encoder outputs.

### Text Encoder Output Pre-caching / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Text encoder output pre-caching is required. Create the cache using the following command:

```bash
python kandinsky5_cache_text_encoder_outputs.py \
    --dataset_config path/to/dataset.toml \
    --text_encoder_qwen path/to/text_encoder \
    --text_encoder_clip path/to/text_encoder2 \
    --batch_size 4
```

Adjust `--batch_size` according to your available VRAM.

For additional options, use `python kandinsky5_cache_text_encoder_outputs.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä½¿ç”¨å¯èƒ½ãªVRAMã«åˆã‚ã›ã¦ `--batch_size` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

### Latent Pre-caching / latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Latent pre-caching is required. Create the cache using the following command:

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors
```

If you're running low on VRAM, lower the `--batch_size`.

For additional options, use `python kandinsky5_cache_latents.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--batch_size`ã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## Training / å­¦ç¿’

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
    kandinsky5_train_network.py \
    --mixed_precision bf16 \
    --dataset_config path/to/dataset.toml \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_pretrain_5s.safetensors \
    --text_encoder_qwen path/to/text_encoder \
    --text_encoder_clip path/to/text_encoder2 \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --fp8_base \
    --blocks_to_swap 10 \
    --flash_attn \
    --gradient_checkpointing \
    --max_data_loader_n_workers 1 \
    --persistent_data_loader_workers \
    --learning_rate 1e-4 \
    --optimizer_type AdamW8Bit \
    --optimizer_args "weight_decay=0.001" "betas=(0.9,0.95)" \
    --max_grad_norm 1.0 \
    --lr_scheduler constant_with_warmup \
    --lr_warmup_steps 100 \
    --network_module networks.lora_kandinsky \
    --network_dim 16 \
    --network_alpha 16 \
    --timestep_sampling shift \
    --discrete_flow_shift 5.0 \
    --output_dir path/to/output \
    --output_name my_lora \
    --save_every_n_epochs 5 \
    --max_train_epochs 50 \
    --scheduler_scale 10.0
```

The training settings are experimental. Appropriate learning rates, training steps, timestep distribution, etc. are not yet fully determined. Feedback is welcome.

For additional options, use `python kandinsky5_train_network.py --help`.

### Key Options / ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration (architecture, attention type, resolution, sampling parameters). See Available Tasks above.
- `--dit`: Path to DiT checkpoint. **Overrides the task's default checkpoint path.** You can use any compatible checkpoint (SFT, pretrain, or your own) with any task config as long as the architecture matches.
- `--vae`: Path to VAE checkpoint (overrides task default)
- `--network_module`: Use `networks.lora_kandinsky` for Kandinsky5 LoRA

**Note**: The `--task` option only sets the model architecture and parameters, not the weights. Use `--dit` to specify which checkpoint to load. For example, you can train a pretrain checkpoint using `--task k5-lite-t2v-5s-sd --dit path/to/pretrain.safetensors`.

**æ³¨æ„**: `--task`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’è¨­å®šã—ã€é‡ã¿ã¯è¨­å®šã—ã¾ã›ã‚“ã€‚`--dit`ã§èª­ã¿è¾¼ã‚€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ã€`--task k5-lite-t2v-5s-sd --dit path/to/pretrain.safetensors`ã®ã‚ˆã†ã«ã€pretrainãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

### Memory Optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

`--gradient_checkpointing` enables gradient checkpointing to reduce VRAM usage.

`--fp8_base` runs DiT in fp8 mode. This can significantly reduce memory consumption but may impact output quality.

If you're running low on VRAM, use `--blocks_to_swap` to offload some blocks to CPU.

`--gradient_checkpointing_cpu_offload` can be used to offload activations to CPU when using gradient checkpointing. This must be used together with `--gradient_checkpointing`.

### Attention / ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Use `--sdpa` for PyTorch's scaled dot product attention. Use `--flash_attn` for FlashAttention. Use `--xformers` for xformers.

### Timestep Sampling / ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

You can specify the range of timesteps with `--min_timestep` and `--max_timestep`. See [advanced configuration](./advanced_config.md) for details.

### Sample Generation During Training / å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

Sample generation during training is supported. See [sampling during training](./sampling_during_training.md) for details.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼ˆå®Ÿéš›ã«ã¯ä¸€è¡Œã§å…¥åŠ›ï¼‰ã€‚

å­¦ç¿’è¨­å®šã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚é©åˆ‡ãªå­¦ç¿’ç‡ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†å¸ƒãªã©ã¯ã€ã¾ã å®Œå…¨ã«ã¯æ±ºã¾ã£ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆä¸Šè¨˜ã®åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’å‚ç…§ï¼‰
- `--dit`: DiTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--vae`: VAEãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--network_module`: Kandinsky5 LoRAã«ã¯ `networks.lora_kandinsky` ã‚’ä½¿ç”¨

**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**

`--gradient_checkpointing`ã§gradient checkpointingã‚’æœ‰åŠ¹ã«ã—ã€VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

`--fp8_base`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€DiTãŒfp8ã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚æ¶ˆè²»ãƒ¡ãƒ¢ãƒªã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ãŒã€å“è³ªã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--blocks_to_swap`ã‚’æŒ‡å®šã—ã¦ã€ä¸€éƒ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

`--gradient_checkpointing_cpu_offload`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€gradient checkpointingä½¿ç”¨æ™‚ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚`--gradient_checkpointing`ã¨ä½µç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**

`--sdpa`ã§PyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`--flash_attn`ã§FlashAttentionã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`--xformers`ã§xformersã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

`--min_timestep`ã¨`--max_timestep`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ç¯„å›²ã‚’æŒ‡å®šã§ãã¾ã™ã€‚è©³ç´°ã¯[é«˜åº¦ãªè¨­å®š](./advanced_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

**å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ**

å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”ŸæˆãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## Inference / æ¨è«–

Generate videos using the following command:

```bash
python kandinsky5_generate_video.py \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_pretrain_5s.safetensors \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --text_encoder_qwen path/to/text_encoder \
    --text_encoder_clip path/to/text_encoder2 \
    --blocks_to_swap 10 \
    --offload_dit_during_sampling \
    --fp8_base \
    --dtype bfloat16 \
    --prompt "A cat walks on the grass, realistic style." \
    --negative_prompt "low quality, artifacts" \
    --frames 17 \
    --steps 50 \
    --guidance 5 \
    --scheduler_scale 10 \
    --seed 42 \
    --width 512 \
    --height 512 \
    --output path/to/output.mp4 \
    --lora_weight path/to/lora.safetensors \
    --lora_multiplier 1.0
```

### Options / ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration
- `--prompt`: Text prompt for generation
- `--negative_prompt`: Negative prompt (optional)
- `--output`: Output file path (.mp4 for video, .png for image)
- `--width`, `--height`: Output resolution (defaults from task config)
- `--frames`: Number of frames (defaults from task config)
- `--steps`: Number of inference steps (defaults from task config)
- `--guidance`: Guidance scale (defaults from task config)
- `--seed`: Random seed
- `--fp8_base`: Run DiT in fp8 mode
- `--blocks_to_swap`: Number of blocks to offload to CPU
- `--lora_weight`: Path(s) to LoRA weight file(s)
- `--lora_multiplier`: LoRA multiplier(s)

For additional options, use `python kandinsky5_generate_video.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®š
- `--prompt`: ç”Ÿæˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- `--negative_prompt`: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--output`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå‹•ç”»ã¯.mp4ã€ç”»åƒã¯.pngï¼‰
- `--width`, `--height`: å‡ºåŠ›è§£åƒåº¦ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--frames`: ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--steps`: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--guidance`: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--seed`: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
- `--fp8_base`: DiTã‚’fp8ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
- `--blocks_to_swap`: CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°
- `--lora_weight`: LoRAé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
- `--lora_multiplier`: LoRAä¿‚æ•°

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## FP8 Checkpoints / FP8ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

You can pre-quantize the DiT checkpoint to fp8 format using the provided script:

```bash
python _make_fp8_ckpt.py \
    --input path/to/kandinsky5pro_t2v_sft_5s.safetensors \
    --output path/to/kandinsky5pro_t2v_sft_5s_fp8.safetensors
```

The fp8 checkpoint can be used directly with training and inference scripts. When an fp8 checkpoint is detected, it will be used as-is without re-quantization.

<details>
<summary>æ—¥æœ¬èª</summary>

æä¾›ã•ã‚Œã¦ã„ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€DiTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’fp8å½¢å¼ã«äº‹å‰é‡å­åŒ–ã§ãã¾ã™ã€‚

fp8ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç›´æ¥ä½¿ç”¨ã§ãã¾ã™ã€‚fp8ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæ¤œå‡ºã•ã‚Œã‚‹ã¨ã€å†é‡å­åŒ–ã›ãšã«ãã®ã¾ã¾ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

</details>

## Dataset Configuration / ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š

Dataset configuration is the same as other architectures. See [dataset configuration](./dataset_config.md) for details.

```toml
[general]
enable_bucket = true
bucket_no_upscale = false

# Image dataset example
[[datasets]]
image_directory = "path/to/images"
cache_directory = "path/to/images/cache"
resolution = [512, 512]
batch_size = 1
num_repeats = 1
caption_extension = ".txt"

# Video dataset example
[[datasets]]
video_directory = "path/to/videos"
cache_directory = "path/to/videos/cache"
resolution = [256, 256]
batch_size = 1
num_repeats = 1
frame_extraction = "head"
target_frames = [17]
caption_extension = ".txt"
```

Note: `target_frames` values must follow the `N*4+1` pattern (1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, ...).

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã¯ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨åŒã˜ã§ã™ã€‚è©³ç´°ã¯[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š](./dataset_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆTOMLã®å½¢å¼ã¯ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨åŒã˜ã§ã™ã€‚

æ³¨æ„: `target_frames` ã®å€¤ã¯ `N*4+1` ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ1, 5, 9, 13, 17, 21, 25, 29, 33, ...ï¼‰ã«å¾“ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

</details>
