> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Kandinsky 5

## Overview / æ¦‚è¦

This is an unofficial training and inference script for [Kandinsky 5](https://github.com/ai-forever/Kandinsky-5). The features are as follows:

- fp8 support and memory reduction by block swap
- Inference without installing Flash attention (using PyTorch's scaled dot product attention)
- LoRA training for text-to-video (T2V) models

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>

[Kandinsky 5](https://github.com/ai-forever/Kandinsky-5) ã®éå…¬å¼ã®å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ï¼ˆT2Vï¼‰ãƒ¢ãƒ‡ãƒ«ã®LoRAå­¦ç¿’

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the model weights from the [Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50) on Hugging Face.

### DiT Model / DiTãƒ¢ãƒ‡ãƒ«

Download the DiT checkpoint from one of the following repositories:

**Lite models (2B parameters):**
- [Kandinsky-5.0-T2V-Lite-sft-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-sft-5s) - SFT model, highest quality
- [Kandinsky-5.0-T2V-Lite-sft-10s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-sft-10s) - 10 second videos
- [Kandinsky-5.0-T2V-Lite-pretrain-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-pretrain-5s) - Pretrain model for fine-tuning
- [Kandinsky-5.0-T2V-Lite-nocfg-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-nocfg-5s) - CFG-distilled, 2x faster
- [Kandinsky-5.0-T2V-Lite-distilled16steps-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Lite-distilled16steps-5s) - Diffusion-distilled, 6x faster
- [Kandinsky-5.0-I2V-Lite-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-I2V-Lite-5s) - Image-to-Video

Download the `.safetensors` file (e.g., `kandinsky5lite_t2v_sft_5s.safetensors`).

### VAE

Kandinsky 5 uses the HunyuanVideo 3D VAE. Download `pytorch_model.pt` from:
https://huggingface.co/tencent/HunyuanVideo/tree/main/hunyuan-video-t2v-720p/vae

### Text Encoders / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

Kandinsky 5 uses Qwen2.5-VL and CLIP for text encoding.

**Qwen2.5-VL**: Download from https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct (or use the path to your local Qwen2.5-VL model)

**CLIP**: Download `clip_l.safetensors` from https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files/text_encoders

### Directory Structure / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

Place them in your chosen directory structure:

```
weights/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ kandinsky5lite_t2v_sft_5s.safetensors
â”œâ”€â”€ vae/
â”‚   â””â”€â”€ pytorch_model.pt
â”œâ”€â”€ text_encoder/
â”‚   â””â”€â”€ (Qwen2.5-VL files)
â””â”€â”€ text_encoder2/
    â””â”€â”€ clip_l.safetensors
```

<details>
<summary>æ—¥æœ¬èª</summary>

Hugging Faceã®[Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50)ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**DiTãƒ¢ãƒ‡ãƒ«**: ä¸Šè¨˜ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰`.safetensors`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**VAE**: Kandinsky 5ã¯HunyuanVideo 3D VAEã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰`pytorch_model.pt`ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**: Qwen2.5-VLã¨CLIPã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

</details>

## Available Tasks / åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯

The `--task` option specifies the model configuration. Available tasks:

**Lite models (2B parameters, publicly available):**

| Task | Description | Resolution | HuggingFace Model |
|------|-------------|------------|-------------------|
| `k5-lite-t2v-5s-sd` | Lite T2V 5s | 480 | Kandinsky-5.0-T2V-Lite-sft-5s |
| `k5-lite-t2v-10s-sd` | Lite T2V 10s | 480 | Kandinsky-5.0-T2V-Lite-sft-10s |
| `k5-lite-i2v-5s-sd` | Lite I2V 5s | 480 | Kandinsky-5.0-I2V-Lite-5s |
| `k5-lite-t2i-hd` | Lite T2I | 1024 | (coming soon) |
| `k5-lite-i2i-hd` | Lite I2I | 1024 | (coming soon) |

**Pro models (19B parameters):**

| Task | Description | Resolution |
|------|-------------|------------|
| `k5-pro-t2v-5s-sd` | Pro T2V 5s SD | 480 |
| `k5-pro-t2v-5s-hd` | Pro T2V 5s HD | 720 |
| `k5-pro-t2v-10s-sd` | Pro T2V 10s SD | 480 |
| `k5-pro-t2v-10s-hd` | Pro T2V 10s HD | 720 |
| `k5-pro-i2v-5s-sd` | Pro I2V 5s SD | 480 |
| `k5-pro-i2v-5s-hd` | Pro I2V 5s HD | 720 |

Note: Pro models (19B) may require access from the Kandinsky team.

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Pre-caching is required before training. This involves caching both latents and text encoder outputs.

### Text Encoder Output Pre-caching / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Text encoder output pre-caching is required. Create the cache using the following command:

```bash
python src/musubi_tuner/kandinsky5_cache_text_encoder_outputs.py \
    --dataset_config path/to/toml \
    --text_encoder_qwen path/to/text_encoder \
    --text_encoder_clip path/to/text_encoder2 \
    --batch_size 4
```

Adjust `--batch_size` according to your available VRAM.

For additional options, use `python src/musubi_tuner/kandinsky5_cache_text_encoder_outputs.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä½¿ç”¨å¯èƒ½ãªVRAMã«åˆã‚ã›ã¦ `--batch_size` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

### Latent Pre-caching / latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Latent pre-caching is required. Create the cache using the following command:

```bash
python src/musubi_tuner/kandinsky5_cache_latents.py \
    --dataset_config path/to/toml \
    --vae path/to/vae/pytorch_model.pt \
    --vae_chunk_size 32 --vae_tiling
```

If you're running low on VRAM, reduce `--vae_spatial_tile_sample_min_size` to around 128 and lower the `--batch_size`.

For additional options, use `python src/musubi_tuner/kandinsky5_cache_latents.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--vae_spatial_tile_sample_min_size`ã‚’128ç¨‹åº¦ã«æ¸›ã‚‰ã—ã€`--batch_size`ã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## Training / å­¦ç¿’

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
    src/musubi_tuner/kandinsky5_train_network.py \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_sft_5s.safetensors \
    --dataset_config path/to/toml \
    --sdpa --mixed_precision bf16 --fp8_base \
    --optimizer_type adamw8bit --learning_rate 2e-4 \
    --gradient_checkpointing \
    --max_data_loader_n_workers 2 --persistent_data_loader_workers \
    --network_module networks.lora_kandinsky --network_dim 32 \
    --timestep_sampling shift --discrete_flow_shift 3.0 \
    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42 \
    --output_dir path/to/output_dir --output_name name-of-lora
```

The training settings are experimental. Appropriate learning rates, training steps, timestep distribution, etc. are not yet fully determined. Feedback is welcome.

For additional options, use `python src/musubi_tuner/kandinsky5_train_network.py --help`.

### Key Options / ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration (see Available Tasks above)
- `--dit`: Path to DiT checkpoint (overrides task default)
- `--vae`: Path to VAE checkpoint (overrides task default)
- `--network_module`: Use `networks.lora_kandinsky` for Kandinsky5 LoRA

### Memory Optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

`--gradient_checkpointing` enables gradient checkpointing to reduce VRAM usage.

`--fp8_base` runs DiT in fp8 mode. This can significantly reduce memory consumption but may impact output quality.

If you're running low on VRAM, use `--blocks_to_swap` to offload some blocks to CPU.

`--gradient_checkpointing_cpu_offload` can be used to offload activations to CPU when using gradient checkpointing. This must be used together with `--gradient_checkpointing`.

### Attention / ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Use `--sdpa` for PyTorch's scaled dot product attention. Use `--flash_attn` for FlashAttention. Use `--xformers` for xformers.

### Timestep Sampling / ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

You can specify the range of timesteps with `--min_timestep` and `--max_timestep`. See [advanced configuration](./advanced_config.md) for details.

### Sample Generation During Training / å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

Sample generation during training is supported. See [sampling during training](./sampling_during_training.md) for details.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼ˆå®Ÿéš›ã«ã¯ä¸€è¡Œã§å…¥åŠ›ï¼‰ã€‚

å­¦ç¿’è¨­å®šã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚é©åˆ‡ãªå­¦ç¿’ç‡ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†å¸ƒãªã©ã¯ã€ã¾ã å®Œå…¨ã«ã¯æ±ºã¾ã£ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆä¸Šè¨˜ã®åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’å‚ç…§ï¼‰
- `--dit`: DiTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--vae`: VAEãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--network_module`: Kandinsky5 LoRAã«ã¯ `networks.lora_kandinsky` ã‚’ä½¿ç”¨

**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**

`--gradient_checkpointing`ã§gradient checkpointingã‚’æœ‰åŠ¹ã«ã—ã€VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

`--fp8_base`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€DiTãŒfp8ã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚æ¶ˆè²»ãƒ¡ãƒ¢ãƒªã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ãŒã€å“è³ªã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--blocks_to_swap`ã‚’æŒ‡å®šã—ã¦ã€ä¸€éƒ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

`--gradient_checkpointing_cpu_offload`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€gradient checkpointingä½¿ç”¨æ™‚ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚`--gradient_checkpointing`ã¨ä½µç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**

`--sdpa`ã§PyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`--flash_attn`ã§FlashAttentionã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`--xformers`ã§xformersã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

`--min_timestep`ã¨`--max_timestep`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ç¯„å›²ã‚’æŒ‡å®šã§ãã¾ã™ã€‚è©³ç´°ã¯[é«˜åº¦ãªè¨­å®š](./advanced_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

**å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ**

å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”ŸæˆãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## Inference / æ¨è«–

Generate videos using the following command:

```bash
python src/musubi_tuner/kandinsky5_generate_video.py \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_sft_5s.safetensors \
    --vae path/to/vae/pytorch_model.pt \
    --text_encoder_qwen path/to/text_encoder \
    --text_encoder_clip path/to/text_encoder2 \
    --prompt "A cat walks on the grass, realistic style." \
    --output path/to/output.mp4 \
    --seed 42 \
    --fp8_base
```

### Options / ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration
- `--prompt`: Text prompt for generation
- `--negative_prompt`: Negative prompt (optional)
- `--output`: Output file path (.mp4 for video, .png for image)
- `--width`, `--height`: Output resolution (defaults from task config)
- `--frames`: Number of frames (defaults from task config)
- `--steps`: Number of inference steps (defaults from task config)
- `--guidance`: Guidance scale (defaults from task config)
- `--seed`: Random seed
- `--fp8_base`: Run DiT in fp8 mode
- `--blocks_to_swap`: Number of blocks to offload to CPU
- `--lora_weight`: Path(s) to LoRA weight file(s)
- `--lora_multiplier`: LoRA multiplier(s)

For additional options, use `python src/musubi_tuner/kandinsky5_generate_video.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®š
- `--prompt`: ç”Ÿæˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- `--negative_prompt`: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--output`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå‹•ç”»ã¯.mp4ã€ç”»åƒã¯.pngï¼‰
- `--width`, `--height`: å‡ºåŠ›è§£åƒåº¦ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--frames`: ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--steps`: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--guidance`: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--seed`: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
- `--fp8_base`: DiTã‚’fp8ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
- `--blocks_to_swap`: CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°
- `--lora_weight`: LoRAé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
- `--lora_multiplier`: LoRAä¿‚æ•°

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## FP8 Checkpoints / FP8ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

You can pre-quantize the DiT checkpoint to fp8 format using the provided script:

```bash
python _make_fp8_ckpt.py \
    --input path/to/kandinsky5pro_t2v_sft_5s.safetensors \
    --output path/to/kandinsky5pro_t2v_sft_5s_fp8.safetensors
```

The fp8 checkpoint can be used directly with training and inference scripts. When an fp8 checkpoint is detected, it will be used as-is without re-quantization.

<details>
<summary>æ—¥æœ¬èª</summary>

æä¾›ã•ã‚Œã¦ã„ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€DiTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’fp8å½¢å¼ã«äº‹å‰é‡å­åŒ–ã§ãã¾ã™ã€‚

fp8ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç›´æ¥ä½¿ç”¨ã§ãã¾ã™ã€‚fp8ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæ¤œå‡ºã•ã‚Œã‚‹ã¨ã€å†é‡å­åŒ–ã›ãšã«ãã®ã¾ã¾ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

</details>

## Dataset Configuration / ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š

Dataset configuration is the same as other architectures. See [dataset configuration](./dataset_config.md) for details.

For Kandinsky5, use `architecture = "kandinsky5"` or `architecture = "kandinsky5_full"` in your dataset TOML file:

```toml
[general]
resolution = [480, 848]
caption_extension = ".txt"
batch_size = 1
enable_bucket = true
bucket_no_upscale = true

[[datasets]]
video_directory = "path/to/videos"
cache_directory = "path/to/cache"
target_frames = [1, 25, 45, 65, 85, 105, 125]
frame_extraction = "head"
architecture = "kandinsky5"
```

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã¯ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨åŒã˜ã§ã™ã€‚è©³ç´°ã¯[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š](./dataset_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

Kandinsky5ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆTOMLãƒ•ã‚¡ã‚¤ãƒ«ã§ `architecture = "kandinsky5"` ã¾ãŸã¯ `architecture = "kandinsky5_full"` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

</details>
