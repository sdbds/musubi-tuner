> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Kandinsky 5

## Overview / æ¦‚è¦

This is an unofficial training and inference script for [Kandinsky 5](https://github.com/ai-forever/Kandinsky-5). The features are as follows:

- fp8 support and memory reduction by block swap
- Inference without installing Flash attention (using PyTorch's scaled dot product attention)
- LoRA training for text-to-video (T2V) and image-to-video (I2V, Pro) models

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>

[Kandinsky 5](https://github.com/ai-forever/Kandinsky-5) ã®éå…¬å¼ã®å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ï¼ˆT2Vï¼‰ãŠã‚ˆã³ç”»åƒã‹ã‚‰å‹•ç”»ï¼ˆI2Vã€Proï¼‰ãƒ¢ãƒ‡ãƒ«ã®LoRAå­¦ç¿’

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the model weights from the [Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50) on Hugging Face.

### DiT Model / DiTãƒ¢ãƒ‡ãƒ«

This document focuses on **Pro** models. The trainer also works with **Lite** models.
æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ **Pro** ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­å¿ƒã«èª¬æ˜ã—ã¾ã™ãŒã€ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¯ **Lite** ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚

Download a Pro DiT `.safetensors` checkpoint from the Kandinsky 5.0 Collection (e.g. `kandinsky5pro_t2v_pretrain_5s.safetensors` or `kandinsky5pro_i2v_sft_5s.safetensors`).

### VAE

Kandinsky 5 uses the HunyuanVideo 3D VAE. Download `diffusion_pytorch_model.safetensors` (or `pytorch_model.pt`) from:
https://huggingface.co/hunyuanvideo-community/HunyuanVideo

### Text Encoders / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

Kandinsky 5 uses Qwen2.5-VL-7B and CLIP for text encoding.

**Qwen2.5-VL-7B**: Download from https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct (or use the path to your local Qwen/Qwen2.5-VL-7B-Instruct model)

**CLIP**: Use the Hugging Face Transformers model `openai/clip-vit-large-patch14`.

Pass either the model ID (e.g., `--text_encoder_clip openai/clip-vit-large-patch14`) or a path to the locally cached snapshot directory.

### Directory Structure / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

Place them in your chosen directory structure:

```
weights/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ kandinsky5pro_t2v_pretrain_5s.safetensors
â”œâ”€â”€ vae/
â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”œâ”€â”€ text_encoder/
â”‚   â””â”€â”€ (Qwen2.5-VL-7B files)
â””â”€â”€ text_encoder2/
    â””â”€â”€ (openai/clip-vit-large-patch14 files)
```

<details>
<summary>æ—¥æœ¬èª</summary>

Hugging Faceã®[Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50)ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ **Proãƒ¢ãƒ‡ãƒ«** ã‚’å‰æã«èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

**DiTãƒ¢ãƒ‡ãƒ«**: ä¸Šè¨˜ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰`.safetensors`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**VAE**: Kandinsky 5ã¯HunyuanVideo 3D VAEã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰`diffusion_pytorch_model.safetensors`ï¼ˆã¾ãŸã¯`pytorch_model.pt`ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**: Qwen2.5-VL-7Bã¨CLIPã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**Qwen2.5-VL-7B**: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã® `Qwen/Qwen2.5-VL-7B-Instruct` ã‚’æŒ‡å®šã—ã¾ã™ï¼‰ã€‚

**CLIP**: Hugging Face Transformersã® `openai/clip-vit-large-patch14` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆãƒ¢ãƒ‡ãƒ«IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸsnapshotãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¾ã™ï¼‰ã€‚

ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

</details>

## List of Kandinsky 5 models / åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯

The `--task` option selects a model configuration (architecture, attention type, resolution, and default parameters).
The DiT checkpoint must be set explicitly via `--dit` (this overrides the task's default checkpoint path).

| # | Task | Checkpoint | Parameters | HF URL |
|---|---|---|---|---|
| 1 | k5-pro-t2v-5s-sd | kandinsky5pro_t2v_sft_5s.safetensors | T2V, 5s, 19B, Pro SFT | [kandinskylab/Kandinsky-5.0-T2V-Pro-sft-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-sft-5s) |
| 2 | k5-pro-t2v-10s-sd | kandinsky5pro_t2v_sft_10s.safetensors | T2V, 10s, 19B, Pro SFT | [kandinskylab/Kandinsky-5.0-T2V-Pro-sft-10s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-sft-10s) |
| 3 | k5-pro-i2v-5s-sd | kandinsky5pro_i2v_sft_5s.safetensors | I2V, 5s, 19B, Pro SFT | [kandinskylab/Kandinsky-5.0-I2V-Pro-sft-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-I2V-Pro-sft-5s) |
| 4 | k5-pro-t2v-5s-sd | kandinsky5pro_t2v_pretrain_5s.safetensors | T2V, 5s, 19B, Pro Pretrain | [kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-5s) |
| 5 | k5-pro-t2v-10s-sd | kandinsky5pro_t2v_pretrain_10s.safetensors | T2V, 10s, 19B, Pro Pretrain | [kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-10s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-10s) |

[Kandinsky 5.0 Video Lite models](https://huggingface.co/collections/kandinskylab/kandinsky-50-video-lite) are technically supported, but were not extensively tested. Community feedback is welcome.

[Kandinsky 5.0 Image Lite models](https://huggingface.co/collections/kandinskylab/kandinsky-50-image-lite) are not supported, but support can be implemented if they get active support from the community.

<details>
<summary>æ—¥æœ¬èª</summary>

`--task` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚¿ã‚¹ã‚¯è¨­å®šï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€attentionã€è§£åƒåº¦ã€å„ç¨®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰ã‚’é¸æŠã—ã¾ã™ã€‚
DiTã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ `--dit` ã§æ˜ç¤ºçš„ã«æŒ‡å®šã§ãã¾ã™ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ã‚¹ã‚’ä¸Šæ›¸ãã—ã¾ã™ï¼‰ã€‚

Kandinsky 5.0 Video Liteãƒ¢ãƒ‡ãƒ«ï¼ˆhttps://huggingface.co/collections/kandinskylab/kandinsky-50-video-liteï¼‰ã¯æŠ€è¡“çš„ã«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ãŒã€ååˆ†ãªå‹•ä½œç¢ºèªã¯ã§ãã¦ã„ã¾ã›ã‚“ã€‚å•é¡ŒãŒã‚ã‚Œã°ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

Kandinsky 5.0 Image Liteãƒ¢ãƒ‡ãƒ«ï¼ˆhttps://huggingface.co/collections/kandinskylab/kandinsky-50-image-liteï¼‰ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ãŒã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‹ã‚‰ã®ç¶™ç¶šçš„ãªè¦æœ›ãƒ»å”åŠ›ãŒã‚ã‚Œã°å¯¾å¿œå¯èƒ½ã§ã™ã€‚

</details>

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Pre-caching is required before training. This involves caching both latents and text encoder outputs.

### Notes for Kandinsky5 / Kandinsky5ã®æ³¨æ„ç‚¹

- You must cache **text encoder outputs** with `kandinsky5_cache_text_encoder_outputs.py` before training.
- `--text_encoder_qwen` / `--text_encoder_clip` are Hugging Face Transformers models: pass a model ID (recommended) or a local HF snapshot directory.
- For I2V tasks, the latent cache stores both first and last frame latents (`latents_image`, always two frames) when running `kandinsky5_cache_latents.py`â€”one cache works for both first-only and first+last conditioning.

<details>
<summary>æ—¥æœ¬èª</summary>

- å­¦ç¿’å‰ã«ã€`kandinsky5_cache_text_encoder_outputs.py` ã«ã‚ˆã‚‹ **ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥** ãŒå¿…é ˆã§ã™ã€‚
- `--text_encoder_qwen` / `--text_encoder_clip` ã¯Hugging Face Transformersã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«IDï¼ˆæ¨å¥¨ï¼‰ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®HF snapshotãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
- I2Vã‚¿ã‚¹ã‚¯ã§ã¯ã€`kandinsky5_cache_latents.py` å®Ÿè¡Œæ™‚ã«æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ latentï¼ˆ`latents_image`ã€å¸¸ã«2ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚1å›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ first / first+last ä¸¡æ–¹ã®ãƒ¢ãƒ¼ãƒ‰ã«å¯¾å¿œã§ãã¾ã™ã€‚

</details>

### Text Encoder Output Pre-caching / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Text encoder output pre-caching is required. Create the cache using the following command:

```bash
python kandinsky5_cache_text_encoder_outputs.py \
    --dataset_config path/to/dataset.toml \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --batch_size 4
```

Adjust `--batch_size` according to your available VRAM.

For additional options, use `python kandinsky5_cache_text_encoder_outputs.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä½¿ç”¨å¯èƒ½ãªVRAMã«åˆã‚ã›ã¦ `--batch_size` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

### Latent Pre-caching / latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Latent pre-caching is required. Create the cache using the following command:

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors
```

For NABLA training, you may want to build NABLA-compatible latent caches:

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --nabla_resize
```

If you're running low on VRAM, lower the `--batch_size`.

For additional options, use `python kandinsky5_cache_latents.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--batch_size`ã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚

NABLAã§å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€NABLAäº’æ›ã®latentã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ï¼š

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --nabla_resize
```

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## Training / å­¦ç¿’

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
    kandinsky5_train_network.py \
    --mixed_precision bf16 \
    --dataset_config path/to/dataset.toml \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_pretrain_5s.safetensors \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --fp8_base \
    --sdpa \
    --gradient_checkpointing \
    --max_data_loader_n_workers 1 \
    --persistent_data_loader_workers \
    --learning_rate 1e-4 \
    --optimizer_type AdamW8Bit \
    --optimizer_args "weight_decay=0.001" "betas=(0.9,0.95)" \
    --max_grad_norm 1.0 \
    --lr_scheduler constant_with_warmup \
    --lr_warmup_steps 100 \
    --network_module networks.lora_kandinsky \
    --network_dim 32 \
    --network_alpha 32 \
    --timestep_sampling shift \
    --discrete_flow_shift 5.0 \
    --output_dir path/to/output \
    --output_name k5_lora \
    --save_every_n_epochs 1 \
    --max_train_epochs 50 \
    --scheduler_scale 10.0
```

For I2V training, switch the task and checkpoint to an I2V preset (e.g., `k5-pro-i2v-5s-sd` with `kandinsky5pro_i2v_sft_5s.safetensors`). The latent cache already stores first and last frame latents (`latents_image`, two frames) when you run `kandinsky5_cache_latents.py`, so the same cache covers both first-only and first+last modesâ€”no extra flags are needed beyond picking an I2V task.

**Note on first+last frame conditioning**: First+last frame training support is experimental. The effectiveness and plausibility of this approach have not yet been thoroughly tested. Feedback and results from community testing are welcome.

The training settings are experimental. Appropriate learning rates, training steps, timestep distribution, etc. are not yet fully determined. Feedback is welcome.

For additional options, use `python kandinsky5_train_network.py --help`.

### Key Options / ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration (architecture, attention type, resolution, sampling parameters). See Available Tasks above.
- `--dit`: Path to DiT checkpoint. **Overrides the task's default checkpoint path.** You can use any compatible checkpoint (SFT, pretrain, or your own) with any task config as long as the architecture matches.
- `--vae`: Path to VAE checkpoint (overrides task default)
- `--network_module`: Use `networks.lora_kandinsky` for Kandinsky5 LoRA

**Note**: The `--task` option only sets the model architecture and parameters, not the weights. Use `--dit` to specify which checkpoint to load.

**æ³¨æ„**: `--task`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’è¨­å®šã—ã€é‡ã¿ã¯è¨­å®šã—ã¾ã›ã‚“ã€‚`--dit`ã§èª­ã¿è¾¼ã‚€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

### Memory Optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

`--gradient_checkpointing` enables gradient checkpointing to reduce VRAM usage.

`--fp8_base` runs DiT in fp8 mode. This can significantly reduce memory consumption but may impact output quality.

If you're running low on VRAM, use `--blocks_to_swap` to offload some blocks to CPU.

`--gradient_checkpointing_cpu_offload` can be used to offload activations to CPU when using gradient checkpointing. This must be used together with `--gradient_checkpointing`.

### Attention / ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Use `--sdpa`, `--flash_attn`, `--flash3`, `--sage_attn`, or `--xformers` to control the attention backend for Kandinsky5.

### Kandinsky5-specific Options / Kandinsky5å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--scheduler_scale`: Overrides the task's scheduler scaling factor. This affects the timestep schedule used in sampling/inference and is also stored in the task config used during training.
- `--offload_dit_during_sampling`: Offloads the DiT model to CPU during sampling (sample generation during training, and in `kandinsky5_generate_video.py`) to reduce peak VRAM usage.
- `--i` / `--image`: Init image path for i2v-style seeding in `kandinsky5_generate_video.py`.

**NABLA attention (training):**

- `--force_nabla_attention`: Force NABLA attention regardless of the task default.
- `--nabla_method`: NABLA binarization method (default `topcdf`).
- `--nabla_P`: CDF threshold (default `0.9`).
- `--nabla_wT`, `--nabla_wH`, `--nabla_wW`: STA window sizes (defaults `11`, `3`, `3`).
- `--nabla_add_sta` / `--no_nabla_add_sta`: Enable/disable STA prior when forcing NABLA.

**NABLA-compatible latent caching:**

- `kandinsky5_cache_latents.py --nabla_resize`: Resizes inputs to the next multiple of 128 before VAE encoding, which helps produce latents compatible with NABLA geometry constraints.

### Sample Generation During Training / å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

Sample generation during training is supported. See [sampling during training](./sampling_during_training.md) for details.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼ˆå®Ÿéš›ã«ã¯ä¸€è¡Œã§å…¥åŠ›ï¼‰ã€‚

æ—¥æœ¬èªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ä¾‹ï¼ˆè‹±èªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨åŒã˜å†…å®¹ï¼‰ï¼š

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
    kandinsky5_train_network.py \
    --mixed_precision bf16 \
    --dataset_config path/to/dataset.toml \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_pretrain_5s.safetensors \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --fp8_base \
    --sdpa \
    --gradient_checkpointing \
    --max_data_loader_n_workers 1 \
    --persistent_data_loader_workers \
    --learning_rate 1e-4 \
    --optimizer_type AdamW8Bit \
    --optimizer_args "weight_decay=0.001" "betas=(0.9,0.95)" \
    --max_grad_norm 1.0 \
    --lr_scheduler constant_with_warmup \
    --lr_warmup_steps 100 \
    --network_module networks.lora_kandinsky \
    --network_dim 32 \
    --network_alpha 32 \
    --timestep_sampling shift \
    --discrete_flow_shift 5.0 \
    --output_dir path/to/output \
    --output_name k5_lora \
    --save_every_n_epochs 1 \
    --max_train_epochs 50 \
    --scheduler_scale 10.0
```

I2Vã®å­¦ç¿’ã‚’è¡Œã†å ´åˆã¯ã€ã‚¿ã‚¹ã‚¯ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’I2Vå‘ã‘ãƒ—ãƒªã‚»ãƒƒãƒˆã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼ˆä¾‹: `k5-pro-i2v-5s-sd` ã¨ `kandinsky5pro_i2v_sft_5s.safetensors`ï¼‰ã€‚`kandinsky5_cache_latents.py` ã§latentã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹éš›ã«ã€æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ latentï¼ˆ`latents_image`ï¼‰ã‚‚ä¿å­˜ã•ã‚Œã‚‹ãŸã‚ã€I2Vå°‚ç”¨ã®è¿½åŠ ãƒ•ãƒ©ã‚°ã¯ä¸è¦ã§ã™ï¼ˆI2Vã‚¿ã‚¹ã‚¯ã‚’é¸ã¶ã ã‘ã§å‹•ä½œã—ã¾ã™ï¼‰ã€‚

**æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ æ¡ä»¶ä»˜ã‘ã«ã¤ã„ã¦**: æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ å­¦ç¿’ã‚µãƒãƒ¼ãƒˆã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã¨å¦¥å½“æ€§ã¯ã¾ã ååˆ†ã«ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨çµæœã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

å­¦ç¿’è¨­å®šã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚é©åˆ‡ãªå­¦ç¿’ç‡ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†å¸ƒãªã©ã¯ã€ã¾ã å®Œå…¨ã«ã¯æ±ºã¾ã£ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆä¸Šè¨˜ã®åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’å‚ç…§ï¼‰
- `--dit`: DiTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--vae`: VAEãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--network_module`: Kandinsky5 LoRAã«ã¯ `networks.lora_kandinsky` ã‚’ä½¿ç”¨

**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**

`--gradient_checkpointing`ã§gradient checkpointingã‚’æœ‰åŠ¹ã«ã—ã€VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

`--fp8_base`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€DiTãŒfp8ã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚æ¶ˆè²»ãƒ¡ãƒ¢ãƒªã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ãŒã€å“è³ªã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--blocks_to_swap`ã‚’æŒ‡å®šã—ã¦ã€ä¸€éƒ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

`--gradient_checkpointing_cpu_offload`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€gradient checkpointingä½¿ç”¨æ™‚ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚`--gradient_checkpointing`ã¨ä½µç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**

`--sdpa`/`--flash_attn`/`--flash3`/`--sage_attn`/`--xformers`ã¯Kandinsky5ã®attention backendã«é©ç”¨ã•ã‚Œã¾ã™ã€‚

**Kandinsky5å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--scheduler_scale`: ã‚¿ã‚¹ã‚¯ã®`scheduler_scale`ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°/æ¨è«–ã§ä½¿ã†ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å½±éŸ¿ã—ã¾ã™ã€‚
- `--offload_dit_during_sampling`: ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆæ™‚ï¼ˆå­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ãŠã‚ˆã³ `kandinsky5_generate_video.py`ï¼‰ã«DiTã‚’CPUã¸é€€é¿ã—ã€ãƒ”ãƒ¼ã‚¯VRAMã‚’ä¸‹ã’ã¾ã™ã€‚
- `--i` / `--image`: `kandinsky5_generate_video.py` ã§i2vé¢¨ã®åˆæœŸç”»åƒï¼ˆ1ãƒ•ãƒ¬ãƒ¼ãƒ ç›®ã®ã‚·ãƒ¼ãƒ‰ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚

**NABLAã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆå­¦ç¿’ï¼‰**

- `--force_nabla_attention`: ã‚¿ã‚¹ã‚¯è¨­å®šã«é–¢ä¿‚ãªãNABLAã‚’å¼·åˆ¶ã—ã¾ã™ã€‚
- `--nabla_method`: NABLAã®äºŒå€¤åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ `topcdf`ï¼‰ã€‚
- `--nabla_P`: CDFã—ãã„å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ `0.9`ï¼‰ã€‚
- `--nabla_wT`, `--nabla_wH`, `--nabla_wW`: STAã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ `11`, `3`, `3`ï¼‰ã€‚
- `--nabla_add_sta` / `--no_nabla_add_sta`: STA priorã®æœ‰åŠ¹/ç„¡åŠ¹ã€‚

**NABLAäº’æ›latentã‚­ãƒ£ãƒƒã‚·ãƒ¥**

- `kandinsky5_cache_latents.py --nabla_resize`: VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å‰ã«å…¥åŠ›ã‚’128ã®å€æ•°ã¸ãƒªã‚µã‚¤ã‚ºã—ã€NABLAã®å¹¾ä½•æ¡ä»¶ã«åˆã†latentã‚’ç”Ÿæˆã—ã‚„ã™ãã—ã¾ã™ã€‚

**å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ**

å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”ŸæˆãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## Inference / æ¨è«–

Generate videos using the following command:

```bash
python kandinsky5_generate_video.py \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_pretrain_5s.safetensors \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --offload_dit_during_sampling \
    --fp8_base \
    --dtype bfloat16 \
    --prompt "A cat walks on the grass, realistic style." \
    --negative_prompt "low quality, artifacts" \
    --frames 17 \
    --steps 50 \
    --guidance 5 \
    --scheduler_scale 10 \
    --seed 42 \
    --width 512 \
    --height 512 \
    --output path/to/output.mp4 \
    --lora_weight path/to/lora.safetensors \
    --lora_multiplier 1.0
```

### Options / ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration
- `--prompt`: Text prompt for generation
- `--negative_prompt`: Negative prompt (optional)
- `--output`: Output file path (.mp4 for video, .png for image)
- `--width`, `--height`: Output resolution (defaults from task config)
- `--frames`: Number of frames (defaults from task config)
- `--steps`: Number of inference steps (defaults from task config)
- `--guidance`: Guidance scale (defaults from task config)
- `--seed`: Random seed
- `--fp8_base`: Run DiT in fp8 mode
- `--blocks_to_swap`: Number of blocks to offload to CPU
- `--lora_weight`: Path(s) to LoRA weight file(s)
- `--lora_multiplier`: LoRA multiplier(s)

For additional options, use `python kandinsky5_generate_video.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®š
- `--prompt`: ç”Ÿæˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- `--negative_prompt`: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--output`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå‹•ç”»ã¯.mp4ã€ç”»åƒã¯.pngï¼‰
- `--width`, `--height`: å‡ºåŠ›è§£åƒåº¦ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--frames`: ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--steps`: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--guidance`: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--seed`: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
- `--fp8_base`: DiTã‚’fp8ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
- `--blocks_to_swap`: CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°
- `--lora_weight`: LoRAé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
- `--lora_multiplier`: LoRAä¿‚æ•°

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## Dataset Configuration / ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š

Dataset configuration is the same as other architectures. See [dataset configuration](./dataset_config.md) for details.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã¯ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨åŒã˜ã§ã™ã€‚è©³ç´°ã¯[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š](./dataset_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>
