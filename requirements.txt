torch==2.8.0
torchvision
#xformers

accelerate>=1.6.0
av==14.0.1
bitsandbytes==0.45.4
diffusers>=0.32.1
einops>=0.7.0
huggingface-hub[hf_xet,hf_transfer]>=0.30.2
opencv-python==4.10.0.84
pillow>=11.3.0
safetensors==0.4.5
toml==0.10.2
tqdm>=4.67.1
transformers>=4.46.3
voluptuous==0.15.2

# Wan2.1
ftfy==6.3.1
easydict==1.13

# FLUX.1 Kontext
sentencepiece==0.2.0

# optional dependencies
ascii-magic==2.3.0
matplotlib==3.10.0
tensorboard
wandb
prompt-toolkit==3.0.51

# optimizer
pytorch-optimizer
prodigy-plus-schedule-free
prodigyopt
dadaptation
heavyball>=0.24.4
schedulefree>=1.4.0
torch-optimi>=0.2.1
adam-mini>=1.1.1

hatchling
editables
-e .
triton-windows ; sys_platform == 'win32'
https://github.com/sdbds/flash-attention-for-windows/releases/download/2.8.2/flash_attn-2.8.2+cu128torch2.8.0cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl; sys_platform == 'win32'
https://github.com/sdbds/SageAttention-for-windows/releases/download/2.20_torch280%2Bcu128/sageattention-2.2.0+cu128torch2.8.0-cp311-cp311-win_amd64.whl; sys_platform == 'win32'
git+https://github.com/Dao-AILab/flash-attention; sys_platform == 'linux'
sageattention; sys_platform == 'linux'
psutil; sys_platform == 'linux'