torch==2.7.0
torchvision
#xformers

accelerate>=1.6.0
av==14.0.1
bitsandbytes==0.45.4
diffusers>=0.32.1
einops>=0.7.0
huggingface-hub[hf_xet,hf_transfer]>=0.30.2
opencv-python==4.10.0.84
pillow
safetensors==0.4.5
toml==0.10.2
tqdm>=4.67.1
transformers>=4.46.3
voluptuous==0.15.2

# Wan2.1
ftfy==6.3.1
easydict==1.13

# optional dependencies
ascii-magic==2.3.0
matplotlib==3.10.0
tensorboard
wandb
pillow-avif-plugin

# optimizer
pytorch-optimizer
prodigy-plus-schedule-free
prodigyopt
dadaptation
heavyball>=0.24.4
schedulefree>=1.4.0
torch-optimi>=0.2.1
adam-mini>=1.1.1

triton-windows ; sys_platform == 'win32'
https://github.com/sdbds/flash-attention-for-windows/releases/download/torch270%2Bcu128/flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl ; sys_platform == 'win32'
https://github.com/sdbds/SageAttention-for-windows/releases/download/2.11_torch270%2Bcu128/sageattention-2.1.1+cu128torch2.7.0-cp311-cp311-win_amd64.whl ; sys_platform == 'win32'
flash-attn; sys_platform == 'linux'
sageattention; sys_platform == 'linux'
